---
title: Adottare LLM open-source per soluzioni economicamente convenienti
date: 2025-11-02
description: Un RAG LLM nutrizionale che utilizza modelli open-source e quantizzazione
image: /images/articles/open-source-rag.jpg
alt: RAG LLM nutrizionale che utilizza modelli open source e quantizzazione
tags: ['LLM', 'RAG', 'Llama', 'Qwen', 'quantizzazione']
locale: 'it'
---

### Contesto
Nel mio [articolo precedente](./a-nutritional-llm-assistant-using-rag), ho introdotto lo scopo dei miei studi: capire come sfruttare gli LLM per creare soluzioni econoomicamente convenienti e production-ready. Sebbene la soluzione iniziale fosse funzionale, non era conveniente, poiché si basava su possibili costose richieste REST alle API di OpenAI. Per risolvere questo problema, ho iniziato a esplorare modelli open-source abbastanza piccoli da poter essere facilmente implementati e adottati in un'applicazione reale.

Nei paragrafi seguenti, descriverò i risultati delle mie ricerche, che includono:

- la scelta del modello giusto adatto alle mie esigenze
- l'applicazione della quantizzazione per ottimizzare il modello
- forzare l'output in un formato JSON specifico
- un confronto tra un modello quantizzato con 7B di parametri e un modello non quantizzato con 1B di parametri

### Scelta del modello
Quando si sceglie un modello, uno dei fattori più importanti da considerare è il numero di parametri che ha. I modelli addestrati con un numero maggiore di parametri, spesso nell'ordine dei miliardi, sono generalmente più potenti e possono eseguire una gamma più ampia di compiti, ma richiedono anche più risorse di calcolo, il che può portare a costi più elevati e tempi di esecuzione più lenti.

Per questo progetto, l'obiettivo principale era creare una soluzione economicamente conveniente, quindi ho deciso di concentrarmi su modelli più piccoli con un numero inferiore di parametri. Questi modelli sono più facili da implementare e possono essere eseguiti su hardware meno costoso, rendendoli una scelta più pratica per le applicazioni del mondo reale in cui il budget è un problema.

Nella tabella seguente è possibile vedere che quando i parametri crescono, allora il tempo di esecuzione e lo spazio richiesto in RAM e disco crescono di conseguenza.

| **Modello LLM** | **Parametri (miliardi)** | **Spazio VRAM (inferenza, stima)** | **Spazio su disco (stima)** | **Tempo di esecuzione (latenza)** |
|---|---|---|---|---|
| Llama 3.1 Instruct 1B | 1 B | ~ 2.3 GB | ~ 2.3 GB | Molto basso. Ideale per inferenze leggere su GPU o CPU di base. |
| Qwen 2.5 Instruct 7B | 7 B | ~ 5GB (4 bit) / ~ 15.2 GB (BF16) | ~ 7-13 GB | Basso/Medio. Efficienza simile a Llama 8B, eseguibile su una singola GPU >= 16 GB. |
| Llama 3.1 Instruct 8B | 8 B | 4.9 GB (Q4_K_M) / 16 GB (FP16) | ~ 5-16 GB | Basso/Medio. Altamente efficiente, eseguibile su una singola GPU di fascia media. |
| Llama 3.1 Instruct 70B | 70 B | ~ 40 GB (Q4_K_M) / 141 GB (FP16) | ~ 40-141 GB | Medio/Alto. Richiede GPU di fascia alta o più GPU. |

Per lo scopo di ricerca, ho deciso di utilizzare `Llama 3.1 Instruct 1B` e `Qwen 2.5 Instruct 7B`.

### Quantizzazione del modello

La tabella menziona termini come `Q3_K_L`, `Q4_K_M`, `BF16`, `FP16`, `4 bit` e `8 bit`, che si riferiscono tutti a tecniche di quantizzazione. La <a href="https://huggingface.co/docs/transformers/main/quantization/overview" target="_blank">quantizzazione</a> è un metodo per ridurre la memoria e la potenza di elaborazione richieste dagli LLM. Funziona convertendo i parametri del modello dal loro formato standard a virgola mobile a `32 bit` in tipi di precisione inferiore, come interi a `8 bit` o addirittura a `4 bit`. Questi parametri, noti anche come **weights (pesi)**, sono i valori numerici che il modello apprende durante l'addestramento e che ne definiscono il comportamento.

Un modello standard non quantizzato offre la massima precisione ma richiede una notevole memoria e potenza di calcolo, come mostrato nella tabella. Applicando una quantizzazione a `4 bit` a un modello come `Qwen 7B` o `Llama 8B`, i valori ad alta precisione vengono mappati su un intervallo più compatto e a precisione inferiore. Sebbene ciò possa ridurre leggermente la precisione, il compromesso è un utilizzo di memoria significativamente inferiore e prestazioni più veloci, il che è ottimo quando si implementano modelli su dispositivi con risorse limitate o si cerca di contenere i costi. Seguendo lo scopo della ricerca, ho adottato la quantizzazione a `4 bit` per il modello `Qwen 7B`, che ha portato a uno spazio di memoria occupato di circa 5 GB partendo da circa 15GB.

### Transformer e tokenizzazione

Prima di passare al codice, è importante avere un contesto completo di ciò che leggerete nelle prossime righe di codice. Per questo motivo, devo presentarvi due nuovi concetti fondamentali, due pilastri alla base degli LLM: i **transformer** e i **tokenizer**.

#### Transformer
Al centro degli LLM ci sono i <a href="https://developers.google.com/machine-learning/crash-course/llm/transformers" target="_blank">transformer</a>, un tipo di architettura di rete neurale che eccelle nella gestione di dati sequenziali, come il testo. I transformer sono composti da due parti principali: un codificatore e un decodificatore. Il codificatore elabora il testo di input e ne crea una rappresentazione numerica, mentre il decodificatore utilizza questa rappresentazione per generare il testo di output.

#### Tokenizer
Prima che un transformer possa elaborare il testo, il testo deve essere convertito in un formato che il modello possa comprendere: è qui che entra in gioco la **tokenizzazione**. La tokenizzazione è il processo di scomposizione di un pezzo di testo in unità più piccole, chiamate **token**, che possono essere parole, sottoparole o persino singoli caratteri. Un **tokenizer** è uno strumento responsabile dell'esecuzione di questa tokenizzazione e ha un vocabolario di tutti i token che il modello conosce, mappando ogni token su un ID numerico univoco. Questa sequenza di ID è quindi ciò che viene immesso nel modello transformer. Puoi trovare maggiori informazioni sulla tokenizzazione <a href="https://huggingface.co/learn/llm-course/chapter2/4" target="_blank">su HuggingFace</a>.

### Applicazione della quantizzazione a Qwen

Ora che abbiamo tutti i pezzi, possiamo finalmente comporre il nostro puzzle. Ho creato un apposito <a href="https://colab.research.google.com/drive/1SPlXn66dBq8jSx3RdqEPwQQw9NttRzN3?usp=sharing" target="_blank">Google Colab Notebook</a>, potete facilmente eseguire i passaggi che descriverò da lì e capire meglio come funziona tutto.

Innanzitutto, dobbiamo creare una configurazione per la quantizzazione. Per fare ciò, ho utilizzato una libreria chiamata **BitsAndBytesConfig** ma ce ne sono <a href="https://huggingface.co/docs/transformers/main/quantization/overview" target="_blank">tantissime</a> disponibili per lo stesso scopo, a seconda del metodo adottato e dell'unità di elaborazione supportata (CPU, GPU).
Con questa configurazione, possiamo quindi inizializzare **un modello quantizzato** e il **tokenizer** ottenuto dal modello di base:

```python
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

BASE_MODEL = "qwen/Qwen2.5-7B-Instruct"

# Crea la configurazione
quantization_config = BitsAndBytesConfig(load_in_4bit=True)

# Carica il modello e applica la quantizzazione
quantized_model = AutoModelForCausalLM.from_pretrained(
  BASE_MODEL,
  device_map="auto", # Ciò consente di utilizzare CUDA se disponibile
  quantization_config=quantization_config
)

# Carica il tokenizer del modello
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)
```

### Outlines per output strutturati

Per forzare l'LLM a restituire un output strutturato, ho usato <a href="https://github.com/dottxt-ai/outlines" target="_blank">Outlines</a>, una libreria che impone al modello a conformarsi a una struttura specifica, come uno schema JSON o un modello Pydantic.

Per raggiungere questo obiettivo, bisogna definire prima un modello Pydantic:

```python
from pydantic import BaseModel

class Food(BaseModel):
  protein: float
  carbohydrates: float
  fats: float
  calories: float
  sugar: float
  fiber: float
```

Successivente, creare un generatore che produrrà un output strutturato quando la richiesta viene soddisfatta dal LLM:

```python
from outlines import from_transformers, Generator

generator = Generator(
  from_transformers(quantized_model, tokenizer),
  Food
)
```

Infine, possiamo usare il generatore per ottenere un output strutturato da un prompt:

```python
prompt = """
  Ottieni i dati nutrizionali del seguente ingrediente alimentare: **salmone**.
  Usa il seguente contesto: ...
"""
result = generator(
  prompt,
  max_new_tokens=200
)
```

Ho rilasciato un <a href="https://github.com/federicoibba/nutritional-information-rag/blob/main/services/qwen.py" target="_blank">servizio Qwen</a> su <a href="https://modal.com" target="_blank">modal</a> così che si possa testare quanto detto prima. Per utilizzarlo, è necessario lanciare la seguente cURL:

```bash
curl --location 'https://ibbus93--nutritional-rag-service-qwen-nutritionalragserv-7ae00e.modal.run/' \
--header 'Content-Type: application/json' \
--data-raw '{ 
    "description": "Salmone"
}'
```

Il che porterà a questo risultato:

```json
{
  "protein": 22.56,
  "carbohydrates": 0.0,
  "fats": 5.57,
  "calories": 140,
  "sugar": 0.0,
  "fiber": 0.0
}
```

### Servizio Llama
Ho rilasciato anche un <a href="https://github.com/federicoibba/nutritional-information-rag/blob/main/services/llama.py" target="_blank">servizio Llama</a> che può essere testato come di seguito:

```bash
curl --location 'https://ibbus93--nutritional-rag-service-llama-nutritionalragser-fc918b.modal.run/' \
--header 'Content-Type: application/json' \
--data-raw '{ 
    "description": "Salmone"
}'
```

Anche se i due servizi hanno ricevuto lo stesso input e hanno utilizzato lo stesso database, il servizio Llama ha restituito una risposta diversa:

```json
{
  "protein": 23.19,
  "carbohydrates": 0.0,
  "fats": 12.95,
  "calories": 209,
  "sugars": 0.0,
  "fibre": 0.0
}
```

### Confronto tra modelli

Esaminiamo ora i due modelli, utilizzando la seguente tabella come confronto.

| **Modello** | **Tempo di esecuzione (5 esecuzioni)** | **Memoria RAM allocata** | **Precisione** |
|---|---|---|---|
| Llama 3.1 Instruct 1B | ~ 2.85 secondi | **~ 2.4 GB** | Discutibile |
| Qwen 2.5 Instruct 7B | **~ 1.72 secondi** | ~ 5 GB | **Abbastanza preciso** |

Come previsto, Qwen ha un allocazione di memoria maggiore, ma ha anche un tempo di risposta più rapido, che è un fattore chiave per le applicazioni reali in produzione.

Per quanto riguarda la precisione, c'è una notevole discrepanza tra i due modelli, sia nello schema JSON che nei dati che restituiscono.
Per quanto riguarda lo schema, i modelli con meno parametri sono generalmente meno affidabili. In questo caso, il modello Llama a volte restituisce un formato diverso tra un'esecuzione e l'altra.

Per quanto riguarda i dati restituiti, il prompt utilizzato da entrambi i modelli è il seguente:

```md
Please use only the following context to answer the question.
**Precedence Rule: Always choose the nutritional data for RAW foods if available.**

Get the nutritional data of the following food ingredient: **Salmon fish**.
CONTEXT OPTIONS:
product name: FISH,SALMON,COHO (SILVER),RAW (ALASKA NATIVE), fat: 5.57, carbohydrates: 0.0, proteins: 22.56, calories: 140, sugars: 0.0, fiber: 0.0 
product name: FISH,SALMON,RED,(SOCKEYE),KIPPERED (ALASKA NATIVE), fat: 4.75, carbohydrates: 0.0, proteins: 24.5, calories: 141, sugars: 0.0, fiber: 0.0 
product name: FISH,SALMON,KING,W/ SKN,KIPPERED,(ALASKA NATIVE), fat: 12.95, carbohydrates: 0.0, proteins: 23.19, calories: 209, sugars: 0.0, fiber: 0.0
```

I dati estratti dal database (quindi dal set di dati adottato) forniscono tre diversi campioni per il salmone, ma si può notare che il primo è **RAW (crudo)** e, mentre Qwen lo sta usando come da regola, Llama di solito ignora l'indicazione ricevuta.

### Conclusioni e sfide future

In conclusione, questa ricerca ha dimostrato che è possibile creare soluzioni LLM economicamente convenienti e production-ready utilizzando modelli open-source e quantizzazione.
Sebbene i modelli più piccoli potrebbero non eguagliare sempre la precisione delle loro controparti più grandi, offrono un vantaggio significativo in termini di consumo di risorse e flessibilità di implementazione.
La scelta del modello giusto dipenderà sempre dalle esigenze specifiche dell'applicazione, ma con il giusto approccio è possibile trovare un equilibrio tra prestazioni e costi.

Come sfida futura, sarebbe interessante esplorare altre tecniche di quantizzazione e perfezionare un modello più piccolo su un dominio specifico per vedere se è possibile migliorarne la precisione mantenendo basso il consumo di risorse.

### Bibliografia
- Fodo dell'articolo di <a href="https://unsplash.com/@kat_katerina?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText" target="_blank">Katerina</a> su <a href="https://unsplash.com/photos/opened-brown-wooden-window-FQYCJSqER_0?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText" target="_blank">Unsplash</a>
      
